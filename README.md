# Conspiracy Theory Generator

Most conspiracy theories these days sound like they were generated by a bot using a handful of words - so I figured, why not make one?

Using text scraped from the Wikipedia Category of US Conspiracy Theories, I defined and trained a neural network to generate a conspiracy theory given a string of seed text and desired length to generate.

Here's an example (seed text in *italics*):

> *teddy bears tweeted about their secret plot to* assassinate fidel castro

> *cute puppies conspired with* saddam husseins regime and that saudi royal president muammar gaddafi

To try it yourself, check out my web application!

![image info](example.png)

**Language:** Python <br>
**Libraries/Modules for Standard Things:** os, sys, time, random, numpy, pandas
**Libraries for Scraping/Data Collection:** re, requests, bs4, pandas, numpy <br>
**Libraries for Loss Visualization:** matplotlib, seaborn
**Libraries for NLP:** nltk
**Libraries for Neural Network:** torch, torchtext
**Libraries for Web App:** flask

## **The Important Stuff**

### â–¶ The Web App

Flask was used to deploy the model with Heroku.

### â–¶ Licensing & Use

 - The files in this project are free for appropriate use under the Mozilla Public License 2.0 (see [LICENSE](LICENSE)).

### â–¶ Setup (if you want to run any of it)

Install environment: 

    conda env create -f environment.yml

### â–¶ Files

    ðŸ“¦TextGenerator
    â”£ ðŸ“‚model
    â”ƒ â”£ ðŸ“œmodel.pt
    â”ƒ â”— ðŸ“œvocab
    â”£ ðŸ“‚src
    â”ƒ â”£ ðŸ“œgenerate.py                       # Generate text from seed text
    â”ƒ â”£ ðŸ“œmodel.py                          # Define and train model
    â”ƒ â”£ ðŸ“œneural_network.py                 # Define neural network
    â”ƒ â”£ ðŸ“œprepare_data.py                   # Split data into sequences for training
    â”ƒ â”£ ðŸ“œprocess_text.py                   # Clean text
    â”ƒ â”£ ðŸ“œscrape_source.py                  # Scrape text from Wikipedia
    â”ƒ â”£ ðŸ“œutilities.py                      # Imports and things used in other scripts
    â”ƒ â”— ðŸ“œ__init__.py
    â”£ ðŸ“‚static                              # App styling
    â”ƒ â”£ ðŸ“œbackground.jpg
    â”ƒ â”£ ðŸ“œfavicon.png
    â”ƒ â”— ðŸ“œstyle.css
    â”£ ðŸ“‚templates                           # App html templates
    â”ƒ â”£ ðŸ“œgenerated.html
    â”ƒ â”— ðŸ“œmain.html
    â”£ ðŸ“œ.gitattributes
    â”£ ðŸ“œ.gitignore
    â”£ ðŸ“œapp.py                              # Flask web application
    â”£ ðŸ“œenvironment.yml
    â”£ ðŸ“œexample.PNG                         # Example screenshot of app
    â”£ ðŸ“œProcfile
    â”£ ðŸ“œREADME.md
    â”£ ðŸ“œrequirements.txt
    â”— ðŸ“œruntime.txt

---

## **Data Collection & Preparation**

### â–¶ Text Collection

    scrape_source.py

The Wikipedia [Category: Conspiracy theories in the United States](https://en.m.wikipedia.org/wiki/Category:Conspiracy_theories_in_the_United_States) page was scraped for urls, and those pages were then scraped for all of their text content.

A dataframe consisting of the urls and their text content was saved in (due to large file size, the ```data``` directory is omitted from the repository):

    data/raw/conspiracy_df.csv

### â–¶ Text Processing

    process_text.py

Basic cleaning of text (lowercasing, removing punctuation/non-alphanumeric characters, removing citations of the form '[81]', removing '[citation needed]', etc.) was performed on the text column of the dataframe. This process was actually done a number of times to obtain different datasets (with/without stop words or hyphens). These columns were added to the dataframe created above and the file re-saved. 

### â–¶ Data Preparation

    prepare_data.py

The raw text was split into sequences, each consisting of five words (this can be adjusted to any length). Those sequences were then transformed into 'text' and 'target' sequences, to prepare them for training the model. For each 'text' sequences of five words, the target sequence was the text sequence shifted forward by one word. For example:

    Example data:  {'text': ['district', 'for', 'nine', 'terms', 'downing'], 'target': ['for', 'nine', 'terms', 'downing', 'was']}

A data frame was created for each text collection consisting of all text and target pairs. These were then saved to file in the ```data/processed``` directory, but due to large file sizes, these are omitted from the repository.

---

## **The Neural Network**

### â–¶ Defining the Network

    neural_network.py

The neural network along with functions for counting parameters, training, and evaluating the model were defined.

An LSTM with the option to have bidirectional layers is used, though output is better when bidirectional is set to ```False```, since we're working with just a long string of words.

### â–¶ Defining Parameters and Training the Model

    model.py

Data was split into training and validation sets and batches were prepared. A number of different parameters were tested, but the final model deployed was trained with the following:

    ('BATCH_SIZE', 512)
    ('VOCAB_SIZE', 43352)
    ('EMBEDDING_DIM', 300)
    ('HIDDEN_DIM', 512)
    ('OUTPUT_DIM', 43352)
    ('N_LAYERS', 2)
    ('BIDIRECTIONAL', False)
    ('DROPOUT', 0.5)
    ('N_EPOCHS', 100)

The GloVe pretrained word embedding was used to embed the word sequences. Specifically, ```glove.6B.300d``` was used because it was obtained from Wikipedia, which is where our text data originates, so the embedding dimension is fixed at 300.

For every training run, a folder in the ```models``` directory was created named with a timestamp. Best parameters for the model and optimizer are saved for loading at a later time . Visualization of test and validation loss, as well as the output from the training are saved in the folder for the model. Due to large file sizes, the ```models``` directory is omitted from the repository. The final model used for the web app is saved in the ```model``` directory.

Note: Running in Google Colab may decrease training time (it was faster than the GPU on my machine).

---

## **Generating Text**

    generate.py

Functions are defined that generate the next word from a single word, and that generate a given number of words from a list of words. To generate a single word from a given word, the word is tokenized, translated to integer values via the model's vocabulary, and a tensor is created. The tensor is fed into the model, and the output is transformed into a vector of probabilities via the Softmax function. The five highest probabilites are selected (I wanted to make sure there was variety), and one is selected at random. The index of the chosen probability is translated back into a word via the vocabulary.

An option is given to avoid getting duplicates of the same word, by checking a predicted word against a given list of words (i.e. the seed words being used to generate the text). If it already appears in that list, another of the top five probabilites is randomly selected and that word is used.

---

## **Room for Improvement**

- Work on responsive elements of webapp.
- Text cleaning can be improved - every now and then a nonsensical concatenation of words is generated - probably arising from removal of some punctuation.
- Adjusting the sequence length of data could be interesting. This was not done as preparation of the data was a lengthy process.
- Playing with different training parameters/number of epochs could be enlightening.
- Decreasing the number of trainable parameters would be useful to decrease training time.
- Incoporating part of speech tagging to our data may help improve the quality (readability and sensibility) of our output.
- We can play around with generating other text, like Shakespeare, Dr. Seuss, or tweets based off of a particular user.

---

## *Resources*

A number of resources were consulted in making this project.

- [PyTorch Sentiment Analysis tutorials](https://github.com/bentrevett/pytorch-sentiment-analysis): for learning how to use PyTorch with text data.

- [Upgraded Sentiment Analysis tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb): formed the basis for my bidirectional LSTM network.

- [Build a Natural Language Generation (NLG) System using PyTorch](https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/): for understanding how to prepare data (creating text and target sequences) and predict words after training.

- [Beginners Guide to Text Generation (Pytorch)](https://www.kaggle.com/ab971631/beginners-guide-to-text-generation-pytorch])

- [These](https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm) [different](https://minimaxir.com/2018/05/text-neural-networks/) [posts](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw): for understanding how to choose dimensions.

- [Deploying PyTorch in Python via a REST API with Flask](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/6c042f3d39855d2a2de414758e5f9836/flask_rest_api_tutorial.ipynb#scrollTo=Tsp_cU5B0NFm): for learning how to build a Flask app.

- [Deploying a Flask Application to Heroku](https://stackabuse.com/deploying-a-flask-application-to-heroku/) and [Deploying Flask app on Heroku using GitHub](https://dev.to/lordofdexterity/deploying-flask-app-on-heroku-using-github-50nh): for learning how to deploy Flask app via GitHub/Heroku.

- [Deploy Git subdirectory to Heroku](https://medium.com/@shalandy/deploy-git-subdirectory-to-heroku-ea05e95fce1f)